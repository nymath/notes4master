\documentclass[12pt]{book}
\include{template/Doneright}
%-----------------------------------------------------------------------------%

\begin{document}

\title{Stochastic Mathematics in Finance}
\author{Nanyi, UIBE}
\maketitle % This actually puts the title, author, and date in.

%$$
%\begin{tikzpicture}[transform shape]
%  %the multiplication with floats is not possible. Thus I split the loop in two.
%  \foreach \number in {1,...,8}{
%      % Computer angle:
%        \mycount=\number
%        \advance\mycount by -1
%  \multiply\mycount by 45
%        \advance\mycount by 0
%      \node[draw,circle,inner sep=0.25cm] (N-\number) at (\the\mycount:5.4cm) {};
%    }
%  \foreach \number in {9,...,16}{
%      % Computer angle:
%        \mycount=\number
%        \advance\mycount by -1
%  \multiply\mycount by 45
%        \advance\mycount by 22.5
%      \node[draw,circle,inner sep=0.25cm] (N-\number) at (\the\mycount:5.4cm) {};
%    }
%  \foreach \number in {1,...,15}{
%        \mycount=\number
%        \advance\mycount by 1
%  \foreach \numbera in {\the\mycount,...,16}{
%    \path (N-\number) edge[->,bend right=3] (N-\numbera)  edge[<-,bend
%      left=3] (N-\numbera);
%  }
%}
%\end{tikzpicture}
%$$



%\begin{figure}[tp]
%	\begin{center}
%		\makeatletter
%		\def\@captype{figure}
%		\makeatother
%		\includegraphics[scale=0.5]{figure/ETH.jpeg}
%		\caption{ETH zurich.} 
%	\end{center}
%\end{figure}

\tableofcontents
\clearpage
\addcontentsline{toc}{chapter}{Foreword}
{\huge {\bf Foreword}}
\mainmatter
%-----------------------------------------------------------------------------%

\chapter{Set Theory}

\section{Zermelo Fraenkel set theory}









\section{Logit gate}


\begin{definition}[Family] \ \\
Suppose $\Gamma$ is an index set, then a family $\{e_k\}_{k\in \Gamma}$ in a set $V$ is a function from $\Gamma$ to $V$.
$$
e : \Gamma \to V,\ k\mapsto e_k.
$$
\end{definition}

\begin{definition}[Union, Intersection, Complement] \ \\
 Suppose $A_k \subset \X$. Then the union of $A_k$ is defined by 
 $$
 \Union{k\in\Gamma}{} A_k := \{ x: \exists k_0 \in \Gamma, x\in A_{k_0}\}
 $$
 $$
 \Intersection{k\in\Gamma}{} A_k := \{ x: \forall k \in \Gamma, x\in A_{k}\}
 $$
 $$
 A^c := \{x: \notin A\}
 $$
 $$
 A \backslash B := \{x: x\in A,\ x \notin B \}
 $$
\end{definition}

\begin{definition}[Map] \ \\
Suppose $X$ and $Y$ are sets. A map $f$ from $X$ to $Y$ is a rule, which can be written by
$$
f: X \to Y,\ x \mapsto f(x),
$$
where $x \in X$ and $f(x) \in Y$.
\end{definition}

\begin{definition}[injection, bijection, surjection] \ \\
Suppose $f$ is a map from $X$ to $Y$. Then $f$ is an injection if for all 
$$
f(x_1) = f(x_2) \implies x_1=x_2.
$$
And $f$ is call a bijection if 
$$
\mathrm{range} f = Y.
$$
\end{definition}


\begin{definition}
	
\end{definition}





\begin{definition}[null space and range] \ \\
\end{definition}








\begin{theorem}[De moivre] \ \\
$$
(\Union{k\in\Gamma}{} A_k)^c =  \Intersection{k\in\Gamma}{} A_k^c
$$
\end{theorem}

\begin{definition}[Collection of set] \ \\
The set $\mathcal F$ whose elements are subsets of $X$ is called a collection of set on $X$.
\end{definition}

\begin{definition} Suppose $\mathcal F$ is a collection of set on $X$. Then 
	\begin{itemize}
		\item $\mathcal F$
	\end{itemize}
\end{definition}


\section{Binary Structure}
\begin{definition}[Binary operation] \ \\
A binary operation on a nonempty set $V$ is a map from $V \times V$ to $V$. In this case, the turple $(V,*)$ is called a binary structure.

\end{definition}

\begin{definition}[Semi-group] \ \\
A binary sturcture $(V,*)$ is called a semi-group if the operation $*$ is commutative. More precisely, $\forall a,b,c \in V$, we have
$$
a*(b*c) = (a*b)*c.
$$
In this case, the order of operation does not matter.
\end{definition}

\begin{definition}[Identity] \ \\
Suppose $(V,*)$ is a semi-group, then $e \in V$ is called an identity of $(V,*)$ if 
$$
e*a = a*e = a
$$
holds for all $a\in V$.
\end{definition}

\begin{definition}[Monoid] \ \\
A semigroup that has an indentity is called a monoid.
\end{definition}

\begin{theorem}
The identity on a monoid is unique.
\end{theorem}
\begin{proof}
Suppose $e,\tilde e$ are identities of a monoid $(S,*)$, then
$$
e = \tilde e *e = e*\tilde e= \tilde e.
$$	
\end{proof}


\begin{definition}[Inverse]
Suppose $(V,*)$ is a monoid. For all $a \in V$, if there exists (another) element $\tilde a\in V$ such that 
$$
a*\tilde a = \tilde a *a =e,
$$
then we say $\tilde a $ is the inverse of $a$.
\end{definition}

\begin{definition}[Group]
A monoid $(V,*)$ is said to be  a group if for all $a \in V$, there exists an inverse of $a$.
\end{definition}

\begin{theorem}
Operation inverse is unique.	
\end{theorem}

\begin{definition}[Abelian Group]
A group $(V,*)$ is Abelian if $\forall a,b \in V$,
$$
a*b=b*a.
$$
	
\end{definition}


\begin{example}[+ in python]
Let $V$ denote the class, list, in python. Then $(V,+)$ is a monoid.  
\end{example}
\begin{proof}
	Suppose $l_1,l_2 \in V$, then
$$
l_1 = [a_1,\cdots,a_n],
$$
$$
l_2 = [b_1,\cdots,b_m].
$$
And
$$
l_1 + l_2 = [a_1,\cdots,a_n,b_1,\cdots,b_m].
$$
Note that the identity is $[]$.
\end{proof}

\begin{remark}\
\begin{itemize}
	\item (character,+) is a monoid.
	\item (list,+) is a monoid.
	\item (float,+) is a Abelian group.
	\item (integer,+) is a Abelian group.
\end{itemize}
\end{remark}























\chapter{Banach Spaces}
We begin this chapter with a quick review of the essentials of metric spaces. Then we extend our results on measurable functions and integration to complex-valued functions. After that, we rapidly review the framework of vector spaces, which allows us to consider natural collections of measurable functions that are closed under addition and scalar multiplication.

Normed vector spaces and Banach spaces, which are introduced in the third section of this chapter, play a hugely important role in modern analysis. Most interest focuses on linear maps on these vector spaces. Key results about linear maps that we develop in this chapter include the Hahn-Banach Theorem, the Open Mapping Theorem, the Closed Graph Theorem, and the Principle of Uniform Boundedness.
\newpage
\section{Metric Space}
\begin{definition}[Metric Space] \ \\
A metric on a nonempty set $V$ is a function $d:V\times V \to [0,\infty)$ such that
\begin{itemize}
	\item $d(f,f)=0$ for all $f\in V$;
	\item $d(f,g)=0$ implies $f=g$;
	\item $d(f,g)=d(g,f$ for all $f,g \in V$;
	\item $d(f,h)\leq d(f,g)+d(g,h)$ for all $f,g,h \in V$. 
\end{itemize}
A metric space is a pair $(V,d)$, where $V$ is a nonempty set and $d$ is a metric on $V$.
\end{definition}

\begin{example}[Distance on $\R^n$] \ \\
Suppose $T$ is a positive self-adjoint operator. That is for all $f \in V$, 
$$
\inner{Tv}{v} = \inner{v}{T^* v} = \inner{v}{Tv}>0.
$$
Then we can define a distance on $\R^n$ by:
$$
d(x,y)^2 = \inner{x-y}{T(x-y)}=(x-y)^\prime \M(T) (x-y).
$$
\end{example}
\begin{example}[Minkowski Distance] \ \\ 
Suppose $X_1,\cdots,X_n$ is a random sample, where $X_1 = \begin{bmatrix}
	X_{11} \\
	X_{12} \\
	\vdots \\
	X_{1n}
\end{bmatrix}$ with mean $\mu$ and covariance matrix $\Sigma$. If $X_i$ are uncorrelated and $\x,\mathbf{y}\in \R^p$, then the distance between $\x$ and $\mathbf{y}$ is given by
$$
d^2(\x,\mathbf{y}) = (\x-\mathbf{y})^\prime \begin{bmatrix}
	\sigma_{11}^{-1} & & & \\
	 & \sigma_{22}^{-1} & & \\
	 & & \ddots & \\
	 & & & \sigma_{pp}^{-1}
\end{bmatrix} (\x-\mathbf{y}).
$$
\end{example}
However, if $X_i$ are correlated, we need to translate them into uncorrelated components and then use the above procedure to define the distance. To be specific, the real spectrum theorem yields that
$$
\Sigma = P\Lambda P^\prime , where \ \Lambda  = diag(\lambda_1,\cdots,\lambda_p).
$$
Thus the distance of $\x$ and $\mathbf{y}$ is defined by
\begin{eqnarray*}
d^2(\x,\mathbf{y}) & = &d^2(P^\prime(\x-\mu),P^\prime (\x-\mathbf{y})) \\
&=& (\x-\mathbf{y})^\prime P \Lambda^{-1}P^\prime(\x-\mathbf{y}) \\
&=& (\x-\mathbf{y})^\prime \Sigma^{-1} (\x - \mathbf{y})
\end{eqnarray*}











\begin{definition}[Open ball] \ \\
Suppose $(V,d)$ is a metric space, $f \in V$, and $r >0$.
\begin{itemize}
	\item The open ball centered at $f$ with radius $r$ is denoted by $B(f,r)$ and id defined by
	$$
	B(f,r) = \{g\in V: d(f,g) < r\}.
	$$
	\item The closed ball centered at $f$ with radius $r$ is denoted by $\overline{B}(f,r)$ and is defined by
	$$
	\overline{B}(f,r) = \{g\in V: d(f,g) \leq r\}.
	$$
\end{itemize}
\end{definition}

\begin{definition}[Open subset] \ \\
A subset $G$ of a metric space $V$ is called open if for all $f\in G$, there exists $r >0$ such that $B(f,r)\subset G$.
\end{definition}

\begin{definition}[Open subset] \ \\
A subset $F$ of a metric space $V$ is called close if $V\backslash F$ is open.
\end{definition}

\begin{definition}[closure] \ \\
Suppose $V$ is a metric space and $E\subset V$. Then the closure of $E$, denoted $\backslash E$, is defined by
$$
\overline{E} = \{g \in V: B(g,\varepsilon)\cap E \neq \emptyset,  \forall \varepsilon > 0\}
$$
\end{definition}

\begin{definition}[Limits in V]
Suppose $V$ is a metric space, $f_1,f_2,\cdots$ is a sequence in $V$, and $f\in V$. Then
	$$
\lim_{k \to \infty}f_k =f \ \text{means} \ \lim_{k \to \infty}d(f_k,f)=0
	$$
\end{definition}

\begin{theorem}[closure] \ \\
Suppose $V$ is a metric space and $E \subset V$. Then
\begin{itemize}
	\item $\overline{E}=\{ \}$
\end{itemize}	
\end{theorem}








\chapter{$L^p$ Spaces}
Fix a measure space $(X, \mathcal{S}, \mu)$ and a positive number $p$. We begin this chapter by looking at the vector space of measurable functions $f: X \rightarrow \mathbf{F}$ such that
$$
\int|f|^p d \mu<\infty .
$$
Important results called Holder's inequality and Minkowski's inequality help us investigate this vector space. A useful class of Banach spaces appears when we identify functions that differ only on a set of measure 0 and require $p \geq 1$.
\newpage
Suppose $(\X,\mathcal{S},\mu)$ is measure space, $0<p<\infty$.
\begin{definition}
	
\end{definition}


\begin{theorem}
Suppose $\mathbb{S}=\{\varphi: \varphi \ \text{is simple and } \ \varphi \in L^1(\X,\mathcal S,\mu) \}$. Then $\mathbb{S}$ is dense in $L^1(\X,\mathcal S,\mu)\}$.
\end{theorem}


\include{chapter/Hilbert}
\include{chapter/measure}



\chapter{Integration}
\section{Integration with Respect to a Measure}
\section{Limits of integral and Integral of Limits}
\section{LS Integral}
\section{Iterated Integrals}

\include{chapter/Probability}





















\chapter{Fourier Analysis}
This chapter uses Hilbert space theory to motivate the introduction of Fourier coefficients and Fourier series. The classical setting applies these concepts to functions defined on bounded intervals of the real line. However, the theory becomes easier and cleaner when we instead use a modern approach by considering functions defined on the unit circle of the complex plane.

The first section of this chapter shows how consideration of Fourier series leads us to harmonic functions and a solution to the Dirichlet problem. In the second section of this chapter, convolution becomes a major tool for the $L^p$ theory.

The third section of this chapter changes the context to functions defined on the real line. Many of the techniques introduced in the first two sections of the chapter transfer easily to provide results about the Fourier transform on the real line. The highlights of our treatment of the Fourier transform are the Fourier Inversion Formula and the extension of the Fourier transform to a unitary operator on $L^2(\mathbf{R})$.

The vast field of Fourier analysis cannot be completely covered in a single chapter. Thus this chapter gives readers just a taste of the subject. Readers who go on from this chapter to one of the many book-length treatments of Fourier analysis will then already be familiar with the terminology and techniques of the subject.
\newpage
\begin{definition}
$\partial D$ is the unit circle in then complex plane:
$$
\partial D = \{z\in \C: |z|=1 \}
$$
\end{definition}

\begin{definition}[Measurable subsets of $\partial D$]\label{Curve Measure} \ 
\begin{itemize}
	\item A subset set of $\partial D$ is measurable $\{t\in(-\pi,\pi]: \E^{it}\in E \}$ is a Borel subset of $\R$. The collection of all measurable subset if denoted by $\mathbf M$. ($\mathbf M$ is sigma-algebra, as you should verify)
	\item $\sigma$ is the measure on $(\partial D,\mathbf M)$ obtained by transferring Lebesgue measure from .
	$$
	\sigma(E)=\frac{\lambda(\{ t\in(-\pi,\pi]: \E^{it}\in E\})}{2\pi}=\frac{\lambda \circ h^{-1}(E)}{2\pi}	$$
\end{itemize}
\end{definition}

\begin{theorem}
$\mathbf{M}$ is a sigma-algebra and $\sigma$ is indeed a measure on $(\partial D,\mathbf{M})$
\end{theorem}
\begin{proof}
In fact, $\mathbf{M}$ can be written as 
$$
\left\{E: h^{-1}(E)\in B(\R) \right\}.
$$	
Suppose $A \in \mathbf{M}$, then 
$$
h^{-1}(\partial D\backslash A) = h^{-1}(\partial D)\backslash h^{-1}(A),
$$
which implies $h^{-1}(\partial D\backslash A) \in B(\R)$. Also
\end{proof}






















\begin{lemma}
Suppose $f$ is a map from $\R$ to $\C$, E is an element in $\mathbf{M}$, then
$$
\chi_E \circ f = \chi_{f^{-1}(E)},
$$
and
$$
\chi_E \circ f^{-1} = \chi_{f(E)}.
$$
\begin{proof}
	Considering that $\chi_E \circ f (t)=1$ if $f(t)\in E$. Also, $\chi_{f^{-1}(E)}(t)=1$ if $t\in f^{-1}(E)$. In other words, $f(t) \in E$.
\end{proof}
\end{lemma}

\begin{theorem}
Suppose $f \in L^1(\partial D)$
	$$
	\int_{\partial D}f\D \sigma = \int_{(-\pi,\pi]}f(e^{it}) \frac{\D \lambda(t)}{2\pi}
	$$
\end{theorem}
\begin{proof}
Suppose $f=\chi_E$, where $E$ is $\mathbf{M}$-measurable. Then
$$
\int_{\partial D} \chi_E \D \sigma = \sigma(E).
$$ 
On the other hand, suppose $h:(-\pi,\pi] \to \partial D,\ t\mapsto \E^{it}$, then we have
\begin{eqnarray*}
	\int_{(-\pi,\pi]} \chi_E \circ h \frac{\D \lambda}{2\pi} &=& \int_{(-\pi,\pi]}\chi_{h^{-1}(E)} \frac{\D \lambda}{2\pi} \\
	&=& \frac{\lambda \circ h^{-1}(E)}{2\pi}.
\end{eqnarray*}
\end{proof}




\begin{remark}
	
\end{remark}



\chapter{Markov Process}
\section{Multi-dimensional Normal Distribution}
At the start of this chapter, we will make a brief review of Linear Algebra. By the spectrum theorem of operator on $\R$, we conclude that a there exists a orth0normal basis such that the matrix of $T$ is an diagonal matrix. To be specific, a positive defined matrix can be viewed as a positive self-adjoint operator and thus 
\begin{definition}[Random Vector] \ \\
Suppose $(\Omega,\mathcal{F},P)$ is a probability space. A random vector $\X$ is a map from $\Omega$ to $\R^n$, and its $k-th$ slot is in $L^1(\mathcal{F})$. More precisely,
$$
\X(\omega) := \begin{bmatrix}
	X_1(\omega) \\
	X_2(\omega) \\
	\vdots \\
	X_p(\omega)
\end{bmatrix}.
$$
\end{definition}

\begin{definition}[Mean, Corvariance] \ \\
$$
E(\X) := \begin{bmatrix}
	E(X_1) \\
	E(X_2) \\
	\vdots \\
	E(X_p)
\end{bmatrix}, \
\Sigma_{\X}=Cov(\X,\X):=E(\X-E(\X)(\X-E(\X))^\prime
$$	
\end{definition}

\begin{theorem}
For the mean and covariance defined above, we have the following properties:
\begin{itemize}
	\item $E(A\X+B\Y)=AE(\X)+BE(\Y)$.
	\item $E(\X C +\Y D)=E(\X)C+E(\Y)D$.
	\item $Cov(A\X+b,A\X+b)=ACov(\X,\X)A^\prime$.
	\item $E(\X\prime A\X)=\mathrm tr(A\Sigma)+\mu^\prime A \mu$
\end{itemize}
\end{theorem}

\begin{example}
Suppose $X_1,X_2,\cdots,X_p$ are i.i.d. $N(0,1)$. Then the joint density function of them is given by:
$$
f_{\X}(x)=\frac{1}{(2\pi)^{\frac{p}{2}}}\E^{-\frac{1}{2}x^\prime x}.
$$
\end{example}
Suppose $Y_1,Y_2,\cdots,Y_q$ are given by:
$$
\begin{bmatrix}
	Y_1 \\
	Y_2 \\
	\vdots \\
	Y_q
\end{bmatrix} = A
\begin{bmatrix}
	X_1 \\
	X_2 \\
	\vdots \\ 
	X_p
\end{bmatrix} + \mathbf{\mu},
$$
where $A$ is a bijection from $\R^p$ to $\R^p$. Then the joint density function of $Y_1,\cdots,Y_p$ is 
$$
f_\Y(\mathbf{y})= \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\E^{-\frac{1}{2}(\Y-\mathbf{\mu})^\prime (A^{-1})^{\prime}A^{-1}(\Y-\mathbf{\mu})},
$$
where $\Sigma=AA^\prime,\Sigma^{-1}=(A^{\prime})^{-1}A^{-1}$. You should also note that 
$$
\{\mathbf y \in \R^p:\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\E^{-\frac{1}{2}(\mathbf y-\mathbf{\mu})^\prime (A^{-1})^{\prime}A^{-1}(\mathbf y-\mathbf{\mu})} = c \}
$$
is a hyper-ellipse with parameter $\ln ((2\pi)^p|\Sigma|c)$ on $\R^p$. By the spectrum theorem of a positive self-adjoint operator, we have
$$
\Sigma = \lambda_1 e_1 e_1^\prime + \cdots +\lambda_p e_p e_p^\prime,
$$
where $\lambda_1 > \lambda_2 > \cdots > \lambda_p$. Note that 
$$
d(\mathbf y,\mathbf \mu)^2 = \inner{\Sigma(\mathbf y-\mathbf \mu)}{\mathbf y-\mathbf \mu}.
$$
Suppose $\mathbf y = \mathbf \mu + a*e_1$. Then
$$
d(\mathbf y,\mathbf \mu)  = \ln ((2\pi)^p|\Sigma|c)
$$
implies $a = \frac{1}{\sqrt{\lambda_1}} \ln ((2\pi)^p|\Sigma|^pc)$.







\begin{definition}[Moment Generating Function] \ \\
Then moment generating function of a random vector is a map from $\R^p$ to $\R$:
$$
M_\X(\mathbf{t}):=E(\E^{\inner{\mathbf t}{\X}}).
$$
\end{definition}

\begin{theorem}[Properties of mgf] \ \\
$$
M_{A\X+\mathbf{\mu}}(\mathbf t)=\E^{\inner{\mathbf t}{\mathbf \mu}}M_{\X}(A^\star \mathbf t)
$$
\end{theorem}

\begin{definition}
Suppose $X_1,\cdots,X_p$ are i.i.d. $N(0,1)$, then 
$$
M_\X(\mathbf t) = \E^{\frac{1}{2}\inner{\mathbf t}{\mathbf t}}.
$$	
Also, if $\Y \sim N(\mathbf \mu,\Sigma)$, where $\Sigma = AA^\prime$, then 
$$
M_\Y(\mathbf t) = \E^{\inner{\mathbf t}{\mathbf \mu}+\frac{1}{2}\inner{A^\prime \mathbf t}{A^\prime \mathbf t}}=\E^{\inner{\mathbf t}{\mathbf \mu}+\frac{1}{2}\inner{\Sigma \mathbf t}{ \mathbf t}}.
$$
\end{definition}



\begin{definition}
A random vector $\X = (X_1,X_2,\cdots,X_p)$ has the multivarite normal distribution $N(\mathbf{\mu}, \Sigma)$, if its characteristic function is 
$$
E(\E^{i\inner{\U}{\X}}) = \mathrm{exp} \left( i\inner{\U}{\X} - \frac{1}{2} \U^\prime \Sigma \U\right).
$$
\end{definition}

\begin{theorem}[Conditional Distribution of Multinormal distribution] \ \\
Suppose $\X = \begin{bmatrix}
	X^{(1)} \\
	X^{(2)}
\end{bmatrix}$ follows a multi-dimensional distribution $N(\begin{bmatrix}
	\mathbf \mu_1 \\
	\mathbf \mu_2 
\end{bmatrix} , \begin{bmatrix}
	\Sigma_{(1)(1)} & \Sigma_{(1)(2)} \\
	\Sigma_{(2)(1)} & \Sigma_{(2)(2)}
\end{bmatrix})$.
Then $X^{(1)}_{|x^{(2)}=a} \sim N(\mu^{(1)}+\Sigma_{(1)(2)}\Sigma_{(2)(2)}^{-1}(a-\mu^{(2)}),\Sigma_{(1)(1)}-\Sigma_{(1)(2)}\Sigma_{(2)(2)}^{-1}\Sigma_{(2)(1)})$	
\end{theorem}
\begin{proof}
See \href{https://statproofbook.github.io/P/mvn-cond}{Mulcond} for more details.
\end{proof}

\begin{remark}
In that, $X^{(1)}_{|X^{(2)}}-\mu_1$ is the projection of ...
\end{remark}








\begin{theorem}
Suppose $X \sim N(\mathbb \mu,\Sigma)$ , $A \in \mathbb{R}^{m \times n}$	, $\mathbf{b} \in \mathbb{R}^m$. Then 
$$
AX+\mathbf{b} \sim N(A\mu+\mathbf{b},A \Sigma A^\prime)
$$

\end{theorem}

\section{Sample Geometry}



\begin{definition}
A stochastic process $X = \{X_t, t \geq 0\}$ is a famlily of random variables 
$$
X_t: \Omega \to \R, \omega \mapsto X_t(\omega)
$$
defined on a probability space $(\Omega,\mathcal{F},P)$.
\end{definition}
Note that a famlily is function from ...




\begin{definition}
We can also define a stochastic process on a product measure space $(\Omega \times \R_+, \mathcal F \otimes B(\R_+), P\times \lambda)$.
The stochastic process is a measurable function defined on $\Omega \times \R_+$,
$$
X: \Omega \times \R_+ \to \F, \ (t,\omega) \mapsto X(t,\omega).
$$
	
\end{definition}


\begin{definition}
The finite-dimensional marginal distributions of process X is defined by 
$$
\mu_{t_1,\cdots,t_n} = P \circ (X_{t_1},\cdots,X_{t_n})^{-1}.
$$	
Where $(X_{t_1},\cdots,X_{t_n})^{-1}$  is a map from $B(\R)^n$ to $\mathcal F$,
$$
(X_{t_1},\cdots,X_{t_n})^{-1}: \ (B_1,\cdots,B_n) \mapsto \ \bigcap_{i=1}^n X_{t_i}^{-1}(B_i)
$$
\end{definition}

\begin{theorem}
$P_{t_1,\cdots,t_n}$ is a probability measure on $(\R^n,B(\R^n))$.
\end{theorem}

\begin{definition}
For every $\omega \in \Omega$, the mapping 
$$
X(\cdot,\omega):\ t \mapsto X(t,\omega)
$$
is called a trajectory of the process $X$.
\end{definition}
\begin{remark}
Easy to verify that the trajectory of the process $X$ is a measurable function on $(\R,B(\R))$. 
\end{remark}

\begin{definition}
Two processed X and Y are equivalent (or X is a version of Y) if for all $t\geq 0$,
$$
P\{X_t=Y_t\}=1
$$	
\end{definition}
indistinguishable.
\begin{theorem}[Kolmogorov's contiuity theorem]
	
\end{theorem}
\section{Brownian Motion}
\begin{definition}[Brownian Motion] \ \\
A stochastic process $B$ on $(\Omega \times \R_+, \mathcal F \otimes B(\R_+))$ is called a Brownian motion if 
\begin{enumerate}
  \item $B(0,\cdot)=0$ almost surely.
  \item $B$ have independent and stationary increments.
  \item The increment of $B$ has the normal distribution.
  \item $B(\cdot,\omega)$ is continuous for a.s. $\omega\in \Omega$.
\end{enumerate}
\end{definition}

\begin{definition}[Filtration generated by Bronian motion.] \ \\
$$
\sigma(B_t) = \sigma(\bigcup_{B \in B(\R)}\{\omega:B_t(\omega)\in B\})
$$
Clearly $\sigma(B_t) \subset \mathcal F$.
$$
\mathcal{F}_t := \sigma(\bigcup_{s\leq t}\sigma(B_t))
$$
\end{definition}


\begin{theorem}[Construction of Brownian Motion]
	
\end{theorem}

\section{Other topics}
\begin{definition}[Filtration] \ \\
A filtration $\{\mathcal{F}_t\subset \mathcal{F},t \geq 0\}$ is an increasing family $\sigma$-algebras.
	
\end{definition}


\begin{definition}[Adapted Process] \ \\
A stochastic process $X$ is called an adapted if $X(t,\cdot)$ is $\mathcal{F}_t$-measurable.
\end{definition}

\begin{definition}[Markov process] \ \\
An adapted process $X$ is called a Markov process with respect to $\{\mathcal{F}_t\}$ if for all $s,t>0$ and $f$(f is Borel-measurable function),
$$
E(f(X_{s+t})|\mathcal{F}_t) = E(f(X_{s+t})|X_s) 
$$
\end{definition}

\begin{definition}[Transition Probabilities] \ \\
$$
P(t,x,t+s,B) = P(X_{t+s}\in B|X_s=x),
$$
where B is a Borel subset.
\end{definition}

\begin{definition}[Filtration generated by a process] \ \\
Suppose $X$ is a stochastic process. Then the filtration generated by $X$ is defined by
$$
\mathcal{F}_t^X:=\left\{ \right\}
$$

\end{definition}




\chapter{Stochastic Calculus}
\section{Integral with respect to Brownian Motion}
In this chapter, we will the define the Ito integral of some special stochastic process(Progessively measurable).

Recall that we define the stochastic process $X: X\times [0,\infty) \to \R$ to be a $\mathcal F \otimes B([0,\infty))-measurable$ function. 

$B$ is a Brownian motion on $(\Omega \times \R_+, \mathcal F \otimes B(\R_+))$, and $\mathcal{F}_t$ is the filtration generated by Brownian motion.
\begin{definition}
	$X$ is called processively measurable if for all $t\geq0$, the restriction of $X$ to $\Omega \times [0,t]$ is $\mathcal{F}_t \otimes B([0,t])-measurable$.
$$
\mathcal{P} \ \text{is defined to be a $\sigma$-algebra of Set $A \subset \R^+ \times \Omega$ such that $I_A$ is progressively measurable.}
$$

\end{definition}



\begin{example}
Step function $\varphi(t,\omega)= \sum_{j=0}^{m-1}a_j \chi_{(t_j,t_{j+1}]}(t)$ is progressive measurable. Moreover, $\varphi $ is $ \mathcal P - measurable$.
\end{example}

\begin{example}
Simple process $\varphi(t,\omega)= \sum_{j=0}^{m-1}\phi_j \chi_{(t_j,t_{j+1}]}(t)$ is progressive measurable, if $\phi_j$ is $\mathcal{F}_{t_j}-measurable$. Moreover, $\varphi $ is $ \mathcal P - measurable$.
\end{example}
\begin{proof}
for every $B \in B(\R^+)$, we have

\begin{eqnarray*}
\{(s,\omega):\varphi(s,\omega)I_{(0,t]}\in B\} &=& \bigcup_{i=0}^{m-1} ((t_i\wedge t,t_{i+1}\wedge t] \times \phi^{-1}_i(B))	\\
&=& (t_0 \wedge t, t_m \wedge t] \times \bigcup_{i=0}^{m-1} \phi_i^{-1}(B). X
\end{eqnarray*} 
The set above is clearly $B(0,t] \otimes \mathcal{F}_t - measurable$.

\end{proof}

\begin{example}
$B$ is progressively measurable, where $B$ is a Brownian motion.	 Moreover, if $f:\R \to R$ is a borel measurable function, then $f \circ B$ is progressively measurable.
\end{example}
\begin{proof}
$$
\{(s,\omega): B(s,\omega)I_{(0,t](s)}\in D\} = (0,t] \times \bigcap_{r\leq t}B_r^{-1}(D)
$$
and 
$$
\bigcap_{r\leq t}B_r^{-1}(D) \in \mathcal{F}_t
$$
\end{proof}

\begin{definition}
$$
\mathcal{S} := \{\phi: \phi \ \text{is simple process}\}
$$
\end{definition}

\begin{definition} 
	$$
	L^2(\Omega \times \R_+, \mathcal{P},P\times\lambda) = L^2(\mathcal P)
	$$
and $L^2$ norm is defined by
$$
\| X\|= \int_{\Omega \times B(\R^+)} X^2\  \D P \times \lambda.
$$
\end{definition}
and Fubini's theorem yields
$$
\| X \| = \int_\Omega \int_{B(\R^+)} X^2 \ \D \lambda \D P
$$



\begin{definition}[Ito Integral of simple process] \ \\
Suppose $u \in \mathcal S$, $u = \sum_{j=0}^{m-1}\phi_j \chi_{(t_j,t_{j+1}]}$
. The Ito integral of of $u$ is defined by
$$
I(u) = \int_{\R^+} u_t \D B_t := \sum_{j=0}^{m-1} \phi_j(B_{t_{j+1}}-B_{t_j})
$$	
\end{definition}

\begin{remark}
A quick scan of I is 
$$
I(\chi_{(a,b]}) = B_b - B_a 
$$
$$
I(F\chi_{(a,b]}) = F(B_b-B_a)
$$
if $F \in L^2(\Omega)$ and $\mathcal{F}_a-measurable$ 
\end{remark}

Ito integral is a isometry from $\mathcal S$ to $L^2(\Omega,\mathcal{F},P)$.

\begin{theorem}[Properities of Ito Integral on $\mathcal S$] \ \\
Suppose $u\in \mathcal S$, then
\begin{itemize}
	\item I is a linear map from $S$ to $L^2(\Omega)$.
	\item zero mean: $E(I(u))=0$
	\item Isometry Property: $\| u\| = \| I(u)\|$ and thus $\| I\|=1$.
	\item Isometry Property II: $\inner{u}{v}=\inner{I(u)}{I(v)}$
\end{itemize}
\begin{proof}
Suppose $u = \sum_{j=0}^{m-1}\phi_j \chi_{(t_j,t_{j+1}]}$, then 
\begin{eqnarray*}
E(I(u))&=&E(\sum_{j=0}^{m-1} \phi_j(B_{t_{j+1}}-B_{t_j})) \\
&=&E(E(\sum_{j=0}^{m-1} \phi_j(B_{t_{j+1}}-B_{t_j})|\mathcal{F}_{t_j})) \\
&=&0
\end{eqnarray*}
Also note that, 
$$
\| u \| = \int_{B(\R^+)\times \Omega} u \ \D \lambda \times P
$$
\begin{eqnarray*}
	\| u \|^2 &=& \int_{B(\R^+)\times \Omega} u^2 \ \D \lambda \times P \\
&=& \int_{B(\R^+)\times \Omega} \sum_{i=0}^{m-1}\sum_{j=0}^{m-1}\phi_i \phi_j I_{(t_i,t_{i+1}]}I_{(t_j,t_{j+1}]} \ \D \lambda \times P \\
&=& \sum_{i=0}^{m-1}\int \phi_i^2 I_{(t_i,t_{i+1}]} \\
&=& \sum_{i=0}^{m-1} E(\phi_i^2)(t_{i+1}-t_i)
\end{eqnarray*}
\begin{eqnarray*}
\| I(u) \|^2 &=& E(\sum_{j=0}^{m-1} \phi_j(B_{t_{j+1}}-B_{t_j}))^2 \\
&=& E(\sum_{i=0}^{m-1}\sum_{j=0}^{m-1}\phi_i\phi_j(B_{t_{i+1}}-B_{t_i})(B_{t_{j+1}}-B_{t_j})) \\
&=& \sum_{i=0}^{m-1} E(\phi_i^2)(t_{i+1}-t_i)
\end{eqnarray*}
At last, 
\begin{eqnarray*}
	2\inner{I(u)}{I(v)} &=& \inner{I(u)+I(v)}{I(u)+I(v)}-\inner{I(u)}{I(u)}-\inner{I(v)}{I(v)} \\
	&=& \inner{u+v}{u+v}-\inner{u}{u}-\inner{v}{v} \\
	&=& 2\inner{u}{v}
\end{eqnarray*}
\end{proof}
\end{theorem}

\begin{theorem}
$\mathcal S$ is dense in $L^2(\mathcal P)$.
\end{theorem}
\begin{remark}
A consequence of theorem 2.2 is for all $u \in L^2(\mathcal P)$, there exists ${u_n:u_n \in \mathcal S}$, such that 
$$
\lim_{n \to \infty} u_n = u.
$$
Now consider ${I(u_n)}$.
$$
\| I(u_n)-I(u_m)\|_2 = \| u_n - u_m\|_2
$$
and thus $I(u_n)$ is a cauchy sequence in $L^2(\Omega)$. The completence of Hilbert space admits there exists $I(u)$, such that 
$$
\lim_{n \to \infty} \| I(u_n) - I(u) \|_2 = 0
$$
\end{remark}


\begin{theorem}[Properties of Integral defined on $L^2(\mathcal P)$] \ \\
\begin{itemize}
	\item I is a linear map from $S$ to $L^2(\Omega)$.
	\item zero mean: $E(I(u))=0$
	\item Isometry Property I: $\| u\| = \| I(u)\|$
	\item Isometry Property II: $\inner{u}{v}=\inner{I(u)}{I(v)}$
\end{itemize}
\end{theorem}
\begin{proof}
Recall that the continuity is equivalent to boundness for linear maps from a normed vector space to another. Thus $u_n \to u$ implies
$$
\lim_{n\to \infty} I(u_n) = I(u).
$$
For thm 1,
$$
E(I(u)) = E(\lim_{n\to \infty} I(u_n)) = \lim_{n \to \infty}E(I(u_n))=0,
$$	
where the second equality follows from Dominated Convergence Theorem. \\
For thm 2,
$$
\| I(u) \|^2 =  \|\lim_{n\to \infty} I(u_n) \| = \lim_{n \to \infty}\|(I(u_n)\|=\lim_{n \to \infty}\| u_n\|=\| u\|,
$$
A direct consequence of thm 2 yields thm 3. 
\end{proof}




\begin{definition}[Indefinite Ito Integtal]
For $u \in L^2(\mathcal P)$
$$
I(u,a,b) = \int_a^b u_s \D B_s := I(uI_{(a,b]})
$$
$$
I(u,t) = \int_0^t u_s \D B_s := I(uI_{(0,t]})
$$

\end{definition}
\begin{theorem}[Properties of Indefinite Integral]\label{Indef_prop} \ \\
By definition, 
\begin{itemize}
	\item Additivity
	\item Factorization $I(Fu,a,b)=FI(u,a,b)$ if $F$ is $\mathcal{F}_a-measurable$.
	\item Martingale Property
\end{itemize}
\begin{proof}
First suppose $u \in \mathcal{S}$, $u = \sum_{j=0}^{m-1}\phi_j \chi_{(t_j,t_{j+1}]}$, $F$ is $\mathcal{F}_a-measurable$. Then 
$$
FuI_{(a,b]} = \sum_{j=0}^{m-1}F\phi_j \chi_{(a\vee t_j \wedge b, a \vee t_{j+1} \wedge b]}
$$
is a simple process. Thus the inregral of $FuI_{(a,b]}$ is defined by
\begin{eqnarray*}
	I(Fu,a,b) &=& \sum_{j=0}^{m-1}F\phi_j(B_{a\vee t_{j+1} \wedge b}-B_{a\vee t_j \wedge b}) \\
	&=& F\sum_{j=0}^{m-1}\phi_j(B_{a\vee t_{j+1} \wedge b}-B_{a\vee t_j \wedge b}) \\
	&=& FI(u,a,b) 
\end{eqnarray*}

\begin{eqnarray*}
	I(u,t) &=& I(uI_{(0,t]}) \\
	&=&  \sum_{j=0}^{m-1}I(\phi_j \chi_{(t_j \wedge t,t_{j+1} \wedge t]}) \\
	&=& \sum_{j=0}^{m-1}\phi_jI(\chi_{(t_j \wedge t,t_{j+1} \wedge t]}) \\
	&=& \sum_{j=0}^{m-1}\phi_j(B_{t_{j+1} \wedge t}-B_{t_j \wedge t}).
\end{eqnarray*}

\begin{eqnarray*}
	E(I(u,t)|\mathcal{F}_s) &=& \sum_{j=0}^{m-1} E(E(\phi_j(B_{t_{j+1} \wedge t}-B_{t_j \wedge t})|\mathcal{F}_{t_j \vee s})|\mathcal{F}_s) \\
	&=& \sum_{j=0}^{m-1} E(\phi_jE((B_{t_{j+1} \wedge t}-B_{t_j \wedge t})|\mathcal{F}_{t_j \vee s})|\mathcal{F}_s) \\
	&=& \sum_{j=0}^{m-1}\phi_j(B_{t_{j+1}\wedge s}-B_{t_{j}\wedge s}) \\
	&=& I(u,s)
\end{eqnarray*}
Where the last two equation follows from $(t_j \wedge t)\wedge (t_j \vee s) \wedge s=t_j \wedge s$ and $(t_{j+1} \wedge t)\wedge (t_j \vee s) \wedge s=t_j \wedge s$.

Since the conditional expectation and the limit operation are commutative, we conclude that $I(u,t)$ is a martingale with respected to the Filtration generated by Brownian Motion for all $u \in L^2(\mathcal P)$.
\end{proof}
\end{theorem}

\begin{theorem}
	$I(u,t)$ is a square integrable martingale with respect to the filtration generated by Brownian Moton and admits a continuous version.
\end{theorem}

\begin{proof}
The martingale property of $I(u,t)$ follows from \ref{Indef_prop}, and the Isometry property admits that $I(u,t)$ is square integrable when fixing $t$.
\end{proof}



\begin{theorem}
	I(u,t) 
\end{theorem}

\begin{definition}
123	
\end{definition}

\section{Integral with respect to local martingales}


\end{document}